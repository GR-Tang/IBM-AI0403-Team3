{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "# <a href=\"https://colab.research.google.com/github/GR-Tang/IBM-AI0403-Team3/blob/main/TelcoChurnV10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "XPRSCjXJVKvr",
    "outputId": "ae050a64-5d4a-4efd-a971-cd4048e86003"
   },
   "outputs": [],
   "source": [
    "#importing the libraries needed for fiddling with the dataset\n",
    "import pixiedust\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary libraries for later\n",
    "#!pip install imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_tree\n",
    "from xgboost import plot_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from scipy.stats import sem\n",
    "import graphviz\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D1Jll_dlVs3Q",
    "outputId": "4b0c8148-d762-41ef-aafe-947afd9063fb",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#getting the dataset and loading it up\n",
    "raw_df=pixiedust.sampleData('https://raw.githubusercontent.com/GR-Tang/IBM-AI0403-Team3/main/telco-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking and verifying the dataset\n",
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the headers to fiddle with the features\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "zKGjK3INV22i",
    "outputId": "c77cf34f-e9da-4d09-bac8-9774b73af383",
    "pixiedust": {
     "displayParams": {
      "aggregation": "AVG",
      "charttype": "stacked",
      "handlerId": "tableView",
      "keyFields": "Churn",
      "mpld3": "false",
      "rendererId": "matplotlib",
      "valueFields": "PayByBankTransfer,PayByCC,PayByElectronicCheque,PayByMailedCheque"
     }
    }
   },
   "outputs": [],
   "source": [
    "#can skip, for visualisation\n",
    "#display(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can skip, for visualisation\n",
    "#raw_df.groupby('Churn')[['MonthlyContract', 'OneYearContract', 'TwoYearContract']].sum()\n",
    "#raw_df.groupby('Churn')[['PhoneService', 'MultipleLines', 'DeviceProtection']].sum()\n",
    "#raw_df.groupby('Churn')[['InternetService', 'FiberOptic', 'DSL']].sum()\n",
    "#raw_df.groupby('Churn')[['OnlineSecurity', 'OnlineBackup', 'TechSupport']].sum()\n",
    "#raw_df.groupby('Churn')[['StreamingTV', 'StreamingMovies']].sum()\n",
    "#raw_df.groupby('Churn')[['PayByBankTransfer', 'PayByCC', 'PayByElectronicCheque', 'PayByMailedCheque']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot codes in case needed\n",
    "raw_df['SeniorCitizen']=raw_df['SeniorCitizen'].map({1:'Yes', 0:'No'})\n",
    "raw_df['MultipleLines']=raw_df['MultipleLines'].map({'Yes':'Yes', 'No':'No','No phone service':'No'})\n",
    "raw_df['OnlineSecurity']=raw_df['OnlineSecurity'].map({'Yes':'Yes', 'No':'No','No internet service':'No'})\n",
    "raw_df['OnlineBackup']=raw_df['OnlineBackup'].map({'Yes':'Yes', 'No':'No','No internet service':'No'})\n",
    "raw_df['DeviceProtection']=raw_df['DeviceProtection'].map({'Yes':'Yes', 'No':'No','No internet service':'No'})\n",
    "raw_df['TechSupport']=raw_df['TechSupport'].map({'Yes':'Yes', 'No':'No','No internet service':'No'})\n",
    "raw_df['StreamingTV']=raw_df['StreamingTV'].map({'Yes':'Yes', 'No':'No','No internet service':'No'})\n",
    "raw_df['StreamingMovies']=raw_df['StreamingMovies'].map({'Yes':'Yes', 'No':'No','No internet service':'No'})\n",
    "raw_df=pd.get_dummies(raw_df, columns=['gender','Partner', 'SeniorCitizen', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod'], dtype='int64')\n",
    "#keeping Churn as binary class due to it being target feature\n",
    "raw_df['Churn']=raw_df['Churn'].map({'Yes': 1, 'No': 0})\n",
    "raw_df['TotalCharges']=raw_df['TotalCharges'].replace({' ':'0'}).astype(float)\n",
    "raw_df.drop(columns=['customerID'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouping tenure and monthlycharges into bins\n",
    "raw_df['Bintenure']=(raw_df['tenure']/6).apply(np.ceil).astype(\"int64\")\n",
    "raw_df['BinMonthlyCharges']=(raw_df['MonthlyCharges']/10).apply(np.ceil).astype(\"int64\")\n",
    "raw_df['BinTotalCharges']=(raw_df['TotalCharges']/500).apply(np.ceil).astype(\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shifting the target column to position 0 for ease of reference later\n",
    "churn_df = raw_df['Churn']\n",
    "raw_df.drop(columns=['Churn'], inplace = True)\n",
    "raw_df.insert(0, 'Churn', churn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping unwanted columns\n",
    "imbaldrop2bin_df= raw_df.drop(columns=['gender_Male', 'gender_Female','MonthlyCharges','TotalCharges','tenure'])\n",
    "imbaldrop7bin_df=imbaldrop2bin_df.drop(columns=['PhoneService_Yes','PhoneService_No','StreamingTV_Yes','StreamingTV_No','StreamingMovies_Yes','StreamingMovies_No','MultipleLines_Yes','MultipleLines_No','BinTotalCharges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imbaldrop2bin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noticed target data is imbalanced, we can either undersample (randomly reduce negatives to match the number of positives) \n",
    "#or oversample (randomly generate synthetic positives to match the number of negatives)\n",
    "#however, almost all articles points to balancing after splitting into test/train sets. \n",
    "pd.value_counts(raw_df['Churn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the CSV file to use\n",
    "raw_df=imbaldrop2bin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking 10% of data as test data, the rest to train \n",
    "train_df, test_df = train_test_split(raw_df, test_size=0.1, stratify=raw_df['Churn'], random_state=55)\n",
    "\n",
    "#printing to check size\n",
    "print(\"Size of the training dataset = \", train_df.shape)\n",
    "print(\"Size of the testing dataset = \", test_df.shape)\n",
    "\n",
    "#show sample of the dataset to verify\n",
    "print(\"\\n\\nSample of the training dataset \\n\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(train_df['Churn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot and show the current imbalance\n",
    "rcParams['figure.figsize'] = 6,5\n",
    "sns.countplot(x='Churn', data=train_df)\n",
    "plt.title('Imbalanced Churns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#undersampling NOTE - Choose the one below or this, do not use both\n",
    "#shuffle the Dataset. Everyday we're shuffling..shuffling... shuffling... (frac=1 means gimme back all the rows)\n",
    "shuffled_df = train_df.sample(frac=1,random_state=555)\n",
    "\n",
    "#pull out all the churn positives\n",
    "churnPos_df = shuffled_df.loc[shuffled_df['Churn'] == 1]\n",
    "\n",
    "#randomly pick 1682 rows from the ChurnNegatives (majority)\n",
    "churnNeg_df = shuffled_df.loc[shuffled_df['Churn'] == 0].sample(n = 1682, random_state = 55)\n",
    "\n",
    "#joining them back again\n",
    "train_df = pd.concat([churnNeg_df, churnPos_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oversampling NOTE - Choose the one above or this, do not use both\n",
    "sm = SMOTE(sampling_strategy=0.6667, random_state=55)\n",
    "\n",
    "oversam_train_X, oversam_train_Y = sm.fit_sample(train_df.drop('Churn', axis=1), train_df['Churn'])\n",
    "train_df = pd.concat([pd.DataFrame(oversam_train_Y), pd.DataFrame(oversam_train_X)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(train_df['Churn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now plot and show the corrected balance\n",
    "rcParams['figure.figsize'] = 6,5\n",
    "sns.countplot(x='Churn', data=train_df)\n",
    "plt.title('Balanced Churns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define target column for both test and train dataset\n",
    "train_X, train_Y = train_df.iloc[:,1:],train_df.iloc[:,0]\n",
    "test_X, test_Y = test_df.iloc[:,1:],test_df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting the model, parameters used for the first run is to suppress error messages\n",
    "model1 = xgb.XGBClassifier(use_label_encoder=False, verbosity=0)\n",
    "\n",
    "#fitting the model to the training dataset\n",
    "model1.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_model(model, test_X, test_Y, folds):\n",
    "    #calculating \n",
    "    model_preds = model.predict(test_X)\n",
    "    accuracy = accuracy_score(test_Y, model_preds)\n",
    "    model_probs = model.predict_proba(test_X)\n",
    "    model_probs = model_probs[:, 1]\n",
    "    model_auc = roc_auc_score(test_Y, model_probs)\n",
    "    model_fpr, model_tpr, _ = roc_curve(test_Y, model_probs)\n",
    "    model_precision, model_recall, _ = precision_recall_curve(test_Y, model_probs)\n",
    "    model_f1 = f1_score(test_Y, model_preds)\n",
    "    score = cross_val_score(model, train_X, train_Y, cv=folds, scoring='roc_auc')\n",
    " \n",
    "    print('Model: %s\\n' % (model))\n",
    "    print((folds),'Folds Cross Validation ROC AUC Scores: ', (score))\n",
    "    print('Mean ROC AUC score: {0:.3f} (+/-{1:.3f})'.format(np.mean(score), sem(score)))\n",
    "    print('\\nTest Set score:')\n",
    "    print('Accuracy: %.2f' % (accuracy * 100.0))\n",
    "    print('ROC AUC=%.3f' % (model_auc))\n",
    "    print('F1=%.3f' % (model_f1))\n",
    "\n",
    "    \n",
    "    #plotting confusion matrix\n",
    "    rcParams['figure.figsize'] = 6,5\n",
    "    fig, ax = plt.subplots()\n",
    "    cm = confusion_matrix(test_Y, model_preds)\n",
    "    tp = cm[1,1]\n",
    "    tn = cm[0,0]\n",
    "    fp = cm[0,1]\n",
    "    fn = cm[1,0]\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", linewidths=.5, ax = ax)\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.set_xlabel(\"Predicted class\")\n",
    "    ax.set_ylabel(\"Actual class\")\n",
    "    plt.show()\n",
    "    #plot roc/precision/recall\n",
    "    rcParams['figure.figsize'] = 10,8\n",
    "    plt.plot(model_fpr, model_tpr, marker='.', label='ROC')\n",
    "    plt.plot(model_recall, model_precision, marker='.', label='Recall/Precision')\n",
    "    plt.xlabel('Recall\\nFalse Positive Rate')\n",
    "    plt.ylabel('Precision\\nTrue Positive Rate')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    #plot feature importance (since we are only using two models, using try/except)\n",
    "    rcParams['figure.figsize'] = 8,8\n",
    "    feat_importances = pd.Series(model.feature_importances_, index=train_X.columns)\n",
    "    feat_importances.nlargest(38).plot(kind='barh').invert_yaxis()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#calculate mean square error, not sure if useful for binary classification, included for fun \n",
    "#MSE = np.sqrt(mean_squared_error(test_Y, xgb1_pred_Y))\n",
    "#print(\"XGBoost1 MSE: %f\" % (MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualising the decision tree for shits and giggles\n",
    "rcParams['figure.figsize'] = 30,40\n",
    "xgb.plot_tree(model1, rankdir='LR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assess_model(model1,test_X,test_Y,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will take very very long (bob marley:\"I've been watching you~~\") to run due to the various combinations\n",
    "#thought of increasing cores used but refrained, for portability\n",
    "\n",
    "model2 = xgb.XGBClassifier(use_label_encoder=False, verbosity=0)\n",
    "\n",
    "param_grid = {'eta':[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "              'max_depth':[3, 4, 5, 6, 7],\n",
    "              'gamma':[0, 3, 6, 9, 12],\n",
    "              'subsample':[0.1, 0.25, 0.5, 0.75, 1]\n",
    "}\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=55)\n",
    "grid_search = GridSearchCV(model2, param_grid, scoring='roc_auc', cv=kfold)\n",
    "grid_result = grid_search.fit(train_X, train_Y)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best ROC AUC: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"ROC AUC %f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refit the model with new optimised parameters\n",
    "model3 = xgb.XGBClassifier(use_label_encoder=False, verbosity=0, **grid_result.best_params_)\n",
    "\n",
    "#fitting the model to the training dataset\n",
    "model3.fit(train_X, train_Y)\n",
    "\n",
    "#assess the model with optimised parameters\n",
    "assess_model(model3,test_X,test_Y,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting the alternative model and fitting it with train data, using default params \n",
    "model4 = LGBMClassifier()\n",
    "model4.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assess the default LGBM model to get a baseline scoring\n",
    "assess_model(model4,test_X,test_Y,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform gridsearchcv LGBM model using a few parameters, \n",
    "#this process takes very long since it will run all the possible different combinations of the parameters\n",
    "#params which default is better: 'boosting_type':['gbdt', 'rf'],'learning_rate':[0.025, 0.050, 0.075, 0.1, 0.2]\n",
    "\n",
    "model5 = LGBMClassifier(verbosity=-1)\n",
    "\n",
    "param_grid = {'learning_rate':[0.025, 0.05, 0.1, 0.2, 0.3],\n",
    "              'extra_trees':[True, False],\n",
    "              'max_bin':[10, 55, 155, 255, 280],\n",
    "              'max_delta_step':[-1, 10, 20, 30, 50]\n",
    "}\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=55)\n",
    "grid_search = GridSearchCV(model5, param_grid, scoring='roc_auc', cv=kfold)\n",
    "grid_result = grid_search.fit(train_X, train_Y)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best ROC AUC: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"ROC AUC %f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo - fit lgbm model with optimised parameters \n",
    "model6 = LGBMClassifier(verbosity=-1, **grid_result.best_params_)\n",
    "model6.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo - assess updated LGBM model with optimised parameters\n",
    "assess_model(model6,test_X,test_Y,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7 = GradientBoostingClassifier()\n",
    "model7.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo - assess updated LGBM model with optimised parameters\n",
    "assess_model(model7,test_X,test_Y,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform gridsearch for GBclassifier model using a few parameters, \n",
    "#this process takes very long since it will run all the possible different combinations of the parameters\n",
    "\n",
    "model8 = GradientBoostingClassifier()\n",
    "\n",
    "param_grid = {'learning_rate':[0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "              'n_estimators':[80, 90, 100, 110, 120],\n",
    "              'subsample':[0.01, 0.25, 0.5, 0.75, 1.0],\n",
    "              'max_depth':[1, 2, 3, 5, 7]\n",
    "}\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=55)\n",
    "grid_search = GridSearchCV(model8, param_grid, scoring='roc_auc', cv=kfold)\n",
    "grid_result = grid_search.fit(train_X, train_Y)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best ROC AUC: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"ROC AUC %f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model9 = GradientBoostingClassifier(**grid_result.best_params_)\n",
    "model9.fit(train_X, train_Y)\n",
    "assess_model(model9,test_X,test_Y,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pickle\n",
    "\n",
    "#pickling the model\n",
    "f='model.pkl'\n",
    "with open(f,'wb') as file:\n",
    "    pickle.dump(model6, file)\n",
    "loaded_model = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test loading the pickled model\n",
    "ChurnPred = loaded_model.predict([[1,0,1,0,1,0,0,1,1,0,1,0,0,0,1,0,1,1,0,1,0,1,0,1,0,1,0,0,0,1,0,0,0,1,1,6,1]])\n",
    "print('Churn prediction = %d\\n(1=Yes, 0=No)' % (ChurnPred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"./IMVAI0403-DS-Project-7521954d761a.json\"\n",
    "\n",
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = storage_client.create_bucket('telco-churn-bucket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bucket in storage_client.list_buckets():\n",
    "    print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = bucket.blob('model.pkl')\n",
    "blob.upload_from_filename('./model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Prep for deployment on IBM Cloud\n",
    "#IMPORTANT - READ BELOW:\n",
    "#IMPORTANT - ALL CODES BELOW ARE FOR TECH DEMONSTRATION ONLY, DO NOT RUN. CELLS WILL BE MARK DOWN IN FUTURE VERSIONS.\n",
    "#IMPORTANT - READ ABOVE\n",
    "#!pip install ibm-watson-machine-learning\n",
    "from ibm_watson_machine_learning import APIClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#creating the login credentials and getting the IAM token from IBM Cloud\n",
    "# @hidden_cell\n",
    "api_key = \"<removed>\"\n",
    "wml_credentials = {\n",
    "    \"apikey\": api_key,\n",
    "    \"url\": 'https://us-south.ml.cloud.ibm.com'\n",
    "}\n",
    "client = APIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#getting the list of spaces available on my cloud account\n",
    "client.spaces.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#setting the space this client will be working on\n",
    "client.set.default_space(\"163085a0-83fa-4fed-8c4e-07e486aa743c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#verifying my own system environment\n",
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#checking my package version\n",
    "xgb.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#getting a list of software specifications available on IBM cloud \n",
    "#do note that it's not fantastically compatible, maybe coz i'm on still a free account \n",
    "#free stuff is good stuff, at least for learning purposes\n",
    "client.software_specifications.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Storing the trained model in the cloud space together with the training data \n",
    "metadata = {\n",
    "    client.repository.ModelMetaNames.NAME: 'Telco Churn Prediction Model',\n",
    "    client.repository.ModelMetaNames.TYPE: 'scikit-learn_0.22',\n",
    "    client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: \"154010fa-5b3b-4ac1-82af-4d5ee5abbc85\"\n",
    "}\n",
    "published_model = client.repository.store_model(\n",
    "    model=model1,\n",
    "    meta_props=metadata,\n",
    "    training_data=train_X,\n",
    "    training_target=train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#listing the stored model(s) in my repository\n",
    "models_details = client.repository.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#selecting the stored model by UID and deploying it online\n",
    "metadata = {\n",
    "    client.deployments.ConfigurationMetaNames.NAME: \"Deployment of Telco Churn Prediction Model\",\n",
    "    client.deployments.ConfigurationMetaNames.ONLINE: {}\n",
    "}\n",
    "\n",
    "created_deployment = client.deployments.create(\"8597546e-fac9-4cc4-99e0-ae5646f586cc\", meta_props=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#listing the instances of deployed models\n",
    "client.deployments.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#dumping some numbers into the deployed model for predictions\n",
    "scoring_payload = {\"input_data\": [{\"values\": [[0,0,0,0,3,1,1,1,0,0,1,1,1,1,1,0,10,50,1,1,0,0,0,1,0,0]]}]}\n",
    "predictions = client.deployments.score(\"7c6da0f0-ed51-4c0d-8434-0594b3024ca7\", scoring_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMeXaas/68zuWyB3rT88l83",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Test01.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
